# A development stack to emulate the Lakehouse. This is intended for local development only.
#
# Hostname
# --------
#
# An additional entry in host `/etc/hosts` is required:
#
# 127.0.0.1       analytics.localdev
#
# This name is also added as an additional hosts entry for each compose service and mapped to the
# docker network gateway IP. Combined with the Traefik proxy this enables contacting a service using
# the same address regardless of whether it is accessed internally/externally from the compose network.
#
#  Service routes (you'll have to accept the self-signed certificate warning):
#
#  - Keycloak identity provider: https://analytics.localdev:50443/auth
#  - Lakekeeper iceberg catalog UI: https://analytics.localdev:50443/iceberg/ui. Log in with the credentials listed in keycloak/README
#  - Trino query engine: https://analytics.localdev:58443
#  - Superset BI tool: https://analytics.localdev:50443/workspace/playground. Log in with the credentials defined in env-for-testing
#
#
# Tools such as pyiceberg connect to Lakekeeper, retrieve the address of the S3 storage endpoint
# and communicate directly with it via the 'storage-profile.endpoint' defined in
# infra/local/lakekeeper/bootstrap-warehouse.json. To this end MinIO runs on port 59000 in both
# the container and also on the host.
#
#
# To test ELT scripts based on the Python package `dlt`, create and add the following to
# `$HOME/.dlt/secrets.toml`:
#
# [destination.pyiceberg]
# bucket_url = "s3://playground"
#
# [destination.pyiceberg.credentials]
# uri = "https://analytics.localdev/iceberg/catalog"
# warehouse = "playground"
#

# image versions
x-keycloak-image: &keycloak-image quay.io/keycloak/keycloak:26.3
x-openfga-image: &openfga-image openfga/openfga:v1.8
x-lakekeeper-image: &lakekeeper-image quay.io/lakekeeper/catalog:v0.10.0
x-minio-image: &minio-image quay.io/minio/minio:RELEASE.2025-09-07T16-13-09Z
x-minio-mc-image: &minio-mc-image quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
x-postgres-image: &postgres-image postgres:16.9-bookworm
x-superset-image: &superset-image ghcr.io/martyngigg/adp-superset-forked:901412e
x-traefik-image: &traefik-image traefik:v3.5.3
x-trino-image: &trino-image trinodb/trino:477

# environment
x-env-file: &env-file
  - path: ./env-for-testing
    required: true

# network
x-networks: &local-networks
  - local-lakehouse_net
x-extra-hosts: &extra-hosts
  - "analytics.localdev:host-gateway"

# certs volume
x-certs-volume: &certs-volume "mkcert_data:/certs"

x-mkcert-depends: &mkcert-depends
  mkcert:
    condition: service_completed_successfully

# superset options
x-superset-user: &superset-user root
x-superset-depends-on: &superset-depends-on
  superset-redis:
    condition: service_started
  shared-database-createdatabases:
    condition: service_completed_successfully
  trino-createcatalog:
    condition: service_completed_successfully
x-superset-volumes: &superset-volumes
  - ./superset/docker/docker-init.sh:/app/docker/docker-init.sh:ro
  - ./superset/docker/docker-bootstrap-cert.sh:/app/docker/docker-bootstrap-cert.sh:ro
  - ./superset/docker/requirements-local.txt:/app/docker/requirements-local.txt:ro
  - ./superset/docker/pythonpath:/app/docker/pythonpath:ro
  - *certs-volume

services:
  #########################################################
  # Ingress
  #########################################################
  mkcert:
    container_name: local-lakehouse-mkcert
    image: alpine/mkcert
    entrypoint:
      - /bin/sh
      - -c
      - |
        certfile=/certs/analytics.localdev.pem
        if [ ! -f "$$certfile" ]; then
          echo "$$certfile not found. Creating new CA and certificate."
          cd /certs || exit 1
          mkcert -cert-file "$$certfile" -key-file analytics.localdev-key.pem analytics.localdev localhost 127.0.0.1 ::1
          cp "$$(mkcert -CAROOT)"/* .
        else
          echo "$$certfile exists, skipping certificate creation."
        fi
    restart: "no"
    volumes:
      - *certs-volume
    networks: *local-networks

  traefik:
    container_name: local-lakehouse-traefik
    image: *traefik-image
    env_file: *env-file
    depends_on: *mkcert-depends
    healthcheck:
      test: ["CMD-SHELL", "traefik healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 5s
    ports:
      - "58080:80"
      - "50443:443" # Standard TLS entrypoint
      - "58443:8443" # TLS entrypoint for Trino (cannot run on non-root path prefix or without https with auth enabled)
      - "59000:9000" # API entrypoint for minio that cannot run on a non-root path
    volumes:
      - "./traefik/traefik.yml:/etc/traefik/traefik.yml"
      - "./traefik/traefik-dynamic.yml:/etc/traefik/traefik-dynamic.yml"
      - *certs-volume
    networks: *local-networks
    extra_hosts: *extra-hosts

  #########################################################
  # Database shared by all services
  #########################################################
  shared-database:
    container_name: local-lakehouse-shared-database
    image: *postgres-image
    env_file: *env-file
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -p 5432"]
      interval: 30s
      timeout: 10s
      retries: 2
      start_period: 10s
    volumes:
      - "shared_database_data:/var/lib/postgresql/data"
    networks: *local-networks
    extra_hosts: *extra-hosts

  shared-database-createdatabases:
    container_name: local-lakehouse-shared-database-createdatabases
    image: *postgres-image
    env_file: *env-file
    restart: "no"
    depends_on:
      shared-database:
        condition: service_healthy
    entrypoint:
      - /bin/bash
      - -c
      - |
        create_db_if_not_exists() {
          psql -v ON_ERROR_STOP=1 --host=shared-database --username="$$ADPSUPERUSER" <<-EOSQL
            SELECT 'CREATE DATABASE $1' WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = '$1')\\gexec
        EOSQL
        }
        rename_schema() {
          psql -v ON_ERROR_STOP=1 --host=shared-database --username="$$ADPSUPERUSER" $1 <<-EOSQL
            SELECT 'ALTER SCHEMA $2 RENAME TO $3' WHERE NOT EXISTS (select schema_name from information_schema.schemata WHERE schema_name = '$3')\\gexec
        EOSQL
        }
        create_db_if_not_exists $$SHARED_DB__OPENFGA_DB_NAME
        create_db_if_not_exists $$SHARED_DB__LAKEKEEPER_DB_NAME
        create_db_if_not_exists $$SHARED_DB__SUPERSET_DB_NAME
        rename_schema $$SUPERSET_DB_NAME public $$SUPERSET_DB_SCHEMA_NAME
    networks: *local-networks

  #########################################################
  # Keycloak identity provider
  #########################################################
  keycloak:
    container_name: local-lakehouse-keycloak
    image: *keycloak-image
    env_file: *env-file
    volumes:
      - keycloak_data:/opt/keycloak/data
      - "./healthcheck-localhost.sh:/usr/local/bin/healthcheck-localhost.sh:ro"
      - "./keycloak/realms:/opt/keycloak/data/import:ro"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "healthcheck-localhost.sh 8080 $$KC_HTTP_RELATIVE_PATH/realms/master/.well-known/openid-configuration jwks_uri",
        ]
      start_period: 20s
      start_interval: 2s
      interval: 30s
      timeout: 30s
      retries: 5
    command:
      [
        "start-dev",
        "--metrics-enabled=true",
        "--health-enabled=true",
        "--features=token-exchange",
        "--hostname-debug=true",
        "--proxy-headers=xforwarded",
        "--proxy-trusted-addresses=0.0.0.0/0",
        "--import-realm",
        "--verbose",
        "--log-level=INFO",
      ]
    networks: *local-networks
    extra_hosts: *extra-hosts

  #########################################################
  # MinIO - Object store emulating the Ceph object store
  # in SCD. This is exposed directly rather than through traefik
  # as the service needs to be accessible for clients such as
  # PyIcberg.
  #########################################################
  minio:
    container_name: local-lakehouse-minio
    image: *minio-image
    env_file: *env-file
    # Match ports on host & container to use the same address inside and outside the container network
    command:
      ["server", "/data", "--address", ":9000", "--console-address", ":9001"]
    healthcheck:
      test: ["CMD-SHELL", "healthcheck-localhost.sh 9000 /minio/health/live"]
      start_period: 10s
      interval: 30s
      timeout: 30s
      retries: 3
    volumes:
      - "./healthcheck-localhost.sh:/usr/local/bin/healthcheck-localhost.sh:ro"
      - "minio_data:/data"
    networks: *local-networks
    extra_hosts: *extra-hosts

  minio-createbucket:
    container_name: local-lakehouse-minio-create-bucket
    depends_on:
      minio:
        condition: service_healthy
    image: *minio-mc-image
    env_file: *env-file
    entrypoint:
      - /bin/sh
      - -c
      - |
        /usr/bin/mc alias set minio http://minio:9000 "$${MINIO_ROOT_USER}" "$${MINIO_ROOT_PASSWORD}"
        if ! /usr/bin/mc ls minio/"$${MINIO_BUCKET_NAME}"; then
          echo "Bucket $${MINIO_BUCKET_NAME} not found, creating..."
          /usr/bin/mc mb minio/"$${MINIO_BUCKET_NAME}"
          echo "Bucket $${MINIO_BUCKET_NAME} created."
        else
          echo "Found bucket $${MINIO_BUCKET_NAME}"
        fi
    networks: *local-networks
    extra_hosts: *extra-hosts

  #########################################################
  # OpenFGA - Permissioning
  #########################################################
  openfga-migrate:
    container_name: local-lakehouse-openfga-migrate
    image: *openfga-image
    env_file: *env-file
    command: migrate
    depends_on:
      shared-database-createdatabases:
        condition: service_completed_successfully
    networks: *local-networks

  openfga:
    container_name: local-lakehouse-openfga
    image: *openfga-image
    env_file: *env-file
    command: run
    depends_on:
      openfga-migrate:
        condition: service_completed_successfully
    networks: *local-networks
    extra_hosts: *extra-hosts
    healthcheck:
      test: ["CMD", "/usr/local/bin/grpc_health_probe", "-addr=openfga:8081"]
      interval: 10s
      timeout: 30s
      retries: 3

  #########################################################
  # Lakekeeper - Iceberg REST catalog
  #########################################################
  lakekeeper:
    container_name: local-lakehouse-lakekeeper
    image: *lakekeeper-image
    env_file: *env-file
    environment:
      - SSL_CERT_FILE=/certs/rootCA.pem
      - RUST_LOG=info,lakekeeper=info,lakekeeper_bin=info,iceberg_ext=info
    command: ["serve"]
    healthcheck:
      test: ["CMD", "/home/nonroot/lakekeeper", "healthcheck"]
      start_period: 5s
      interval: 30s
      timeout: 30s
      retries: 3
    depends_on:
      <<: *mkcert-depends
      keycloak:
        condition: service_healthy
      lakekeeper-migrate:
        condition: service_completed_successfully
      minio-createbucket:
        condition: service_completed_successfully
      shared-database-createdatabases:
        condition: service_completed_successfully
      traefik:
        condition: service_healthy
    volumes:
      - *certs-volume
    networks: *local-networks
    extra_hosts: *extra-hosts

  lakekeeper-migrate:
    container_name: local-lakehouse-lakekeeper-migrate
    image: *lakekeeper-image
    env_file: *env-file
    restart: "no"
    command: ["migrate"]
    depends_on:
      shared-database-createdatabases:
        condition: service_completed_successfully
      openfga:
        condition: service_healthy
    networks: *local-networks
    extra_hosts: *extra-hosts

  lakekeeper-createwarehouse:
    container_name: local-lakehouse-lakekeeper-createwarehouse
    image: ghcr.io/astral-sh/uv:python3.13-bookworm-slim
    env_file: *env-file
    command:
      - /bin/bash
      - -c
      - >
        uv run /opt/work/bootstrap-warehouse.py
        --token-url "$$KEYCLOAK_REALM/protocol/openid-connect/token"
        --client-id machine-infra --client-secret s3cr3t
        --token-scope lakekeeper
        --initial-admin-id "oidc~e814f4f3-3a9c-49b9-bb8c-bcabcdaab800"
        --log-level=DEBUG
        --warehouse-json "/opt/work/bootstrap-warehouse.json"
        "$$TRAEFIK_BASE_URL/iceberg"
    restart: "no"
    depends_on:
      lakekeeper:
        condition: service_healthy
    volumes:
      - "../ansible-docker/roles/lakekeeper/files/bootstrap-warehouse.py:/opt/work/bootstrap-warehouse.py"
      - "./lakekeeper/bootstrap-warehouse.json:/opt/work/bootstrap-warehouse.json:ro"
      - *certs-volume
    networks: *local-networks
    extra_hosts: *extra-hosts

  #########################################################
  # Trino query engine
  #########################################################
  trino:
    container_name: local-lakehouse-trino
    image: *trino-image
    env_file: *env-file
    volumes:
      - "./trino/etc:/etc/trino:ro"
    # depends_on:
    #   lakekeeper-createwarehouse:
    #     condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-I", "http://localhost:8080/v1/status"]
      start_period: 10s
      interval: 30s
      timeout: 30s
      retries: 2
    networks: *local-networks
    extra_hosts: *extra-hosts

  trino-createcatalog:
    profiles:
      - superset
    container_name: local-lakehouse-trino-create-catalog
    image: *trino-image
    env_file: *env-file
    volumes:
      - "./trino/trino-create-catalog.sql:/docker/trino-create-catalog.sql:ro"
    command:
      - /bin/bash
      - -c
      - |
        function trinocli() {
          trino --server=https://analytics.localdev:58443 --user $$TRINO_USER --password --insecure $@
        }
        MAX_RETRIES=5
        RETRY_COUNT=0
        SLEEP_TIME=2
        while [ $$RETRY_COUNT -lt $$MAX_RETRIES ]; do
          echo Attempt $$((RETRY_COUNT + 1)) of $${MAX_RETRIES}
          if ! trinocli --execute="show catalogs" 2>/dev/null | grep -q 'playground'; then
            echo "Playground catalog not found, creating..."
            if trinocli --file=/docker/trino-create-catalog.sql; then
              echo "Successfully created catalog"
              exit 0
            fi
          else
            echo "Playground catalog already exists"
            exit 0
          fi
          RETRY_COUNT=$$((RETRY_COUNT + 1))
          if [ $$RETRY_COUNT -lt $$MAX_RETRIES ]; then
            echo "Waiting $${SLEEP_TIME} seconds before next attempt..."
            sleep $${SLEEP_TIME}
          fi
        done

        echo "Failed to create catalog after $${MAX_RETRIES} attempts"
        exit 1
    depends_on:
      trino:
        condition: service_healthy
    restart: "no"
    networks: *local-networks
    extra_hosts: *extra-hosts

  #########################################################
  # Superset BI tool
  #########################################################

  superset-redis:
    profiles:
      - superset
    container_name: local-lakehouse-superset-redis
    env_file: *env-file
    image: redis:8
    restart: unless-stopped
    volumes:
      - shared_redis_data:/data
    networks: *local-networks
    extra_hosts: *extra-hosts

  superset-app:
    profiles:
      - superset
    container_name: local-lakehouse-superset-app
    env_file: *env-file
    image: *superset-image
    command: ["/app/docker/docker-bootstrap-cert.sh", "app-gunicorn"]
    user: *superset-user
    restart: unless-stopped
    depends_on:
      <<: *superset-depends-on
      superset-init:
        condition: service_completed_successfully
    volumes: *superset-volumes
    networks: *local-networks
    extra_hosts: *extra-hosts

  superset-init:
    profiles:
      - superset
    container_name: local-lakehouse-superset-init
    env_file: *env-file
    image: *superset-image
    command: ["/app/docker/docker-init.sh"]
    depends_on:
      <<: *superset-depends-on
      shared-database:
        condition: service_healthy
    user: *superset-user
    volumes: *superset-volumes
    networks: *local-networks
    extra_hosts: *extra-hosts
    healthcheck:
      disable: true

  superset-worker:
    profiles:
      - superset
    container_name: local-lakehouse-superset-worker
    env_file: *env-file
    image: *superset-image
    command: ["/app/docker/docker-bootstrap.sh", "worker"]
    restart: unless-stopped
    depends_on:
      <<: *superset-depends-on
      superset-app:
        condition: service_healthy
    user: *superset-user
    volumes: *superset-volumes
    networks: *local-networks
    extra_hosts: *extra-hosts
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "celery -A superset.tasks.celery_app:app inspect ping -d celery@$$HOSTNAME",
        ]

  superset-worker-beat:
    profiles:
      - superset
    container_name: local-lakehouse-superset-worker-beat
    env_file: *env-file
    image: *superset-image
    command: ["/app/docker/docker-bootstrap.sh", "beat"]
    restart: unless-stopped
    depends_on:
      <<: *superset-depends-on
      superset-app:
        condition: service_healthy
    user: *superset-user
    volumes: *superset-volumes
    networks: *local-networks
    extra_hosts: *extra-hosts
    healthcheck:
      disable: true

networks:
  local-lakehouse_net:
    name: local-lakehouse-network

volumes:
  mkcert_data:
    name: local-lakehouse_mkcert_data
  keycloak_data:
    name: local-lakehouse_keycloak_data
  minio_data:
    name: local-lakehouse_minio_data
  shared_database_data:
    name: local-lakehouse_shared_database_data
  shared_redis_data:
    name: local-lakehouse_shared_redis_data

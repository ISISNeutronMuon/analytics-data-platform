# syntax=docker/dockerfile
######################################################################
# Maven stage to fetch the required jar extensions for Spark
# along with their dependencies.
# The main Spark distribution is currently handled directly by
# downloading it has part of the final image stage.
######################################################################
FROM maven:3.9.9-eclipse-temurin-17 AS spark-jars

RUN mkdir /source
COPY pom.xml /source/

WORKDIR /source
RUN mvn dependency:copy-dependencies

######################################################################
# Final image
######################################################################
FROM python:3.10-bookworm

# Arguments
ARG SPARK_DOWNLOAD_BASE_URL=https://archive.apache.org/dist/spark
ARG SPARK_VERSION=3.5.2

# Environment
ENV SPARK_HOME=/opt/spark

# Basic configuration
ENV PATH="${SPARK_HOME}/sbin:${SPARK_HOME}/bin:${PATH}"
RUN apt-get update && \
  apt-get install -y --no-install-recommends \
  sudo \
  curl \
  unzip \
  openjdk-17-jdk \
  build-essential \
  software-properties-common && \
  apt-get clean && \
  rm -rf /var/lib/apt/lists/*

# Dirs
RUN mkdir -p \
  ${SPARK_HOME} \
  ${SPARK_HOME}/logs \
  ${SPARK_HOME}/spark-events
WORKDIR ${SPARK_HOME}

# Spark
RUN curl ${SPARK_DOWNLOAD_BASE_URL}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz -o spark-${SPARK_VERSION}-bin-without-hadoop.tgz \
  && tar xvzf spark-${SPARK_VERSION}-bin-without-hadoop.tgz --directory ${SPARK_HOME} --strip-components 1 \
  && rm -rf spark-${SPARK_VERSION}-bin-without-hadoop.tgz

# Additional Jars
COPY --chown=root:root --from=spark-jars /source/target/dependency/ ${SPARK_HOME}/jars/

# File modes
RUN chmod u+x ${SPARK_HOME}/sbin/* && \
  chmod u+x ${SPARK_HOME}/bin/*

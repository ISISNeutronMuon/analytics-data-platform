# syntax=docker/dockerfile
FROM python:3.12-bookworm

# Versions
ARG SPARK_VERSION=3.5.1
ARG SPARK_MAJOR_VERSION=3.5
ARG ICEBERG_VERSION=1.5.2
ARG POSTGRES_CONNECTOR_VERSION=42.7.3

# Environment
ENV SPARK_HOME={{ spark_container_home }}
ENV IJAVA_CLASSPATH=${SPARK_HOME}/jars/*
ENV IPYTHON_STARTUP_DIR=/root/.ipython/profile_default/startup
ENV LOCAL_BIN=/usr/local/bin
ENV NOTEBOOKS_DIR=/var/lib/jupyter/notebooks
ENV AWS_REGION=us-east-1
ENV AWS_ACCESS_KEY_ID=minioadmin
ENV AWS_SECRET_ACCESS_KEY=minioadmin
ENV AWS_REGION=us-east-1

# Basic configuration
ENV PATH="${SPARK_HOME}/sbin:${SPARK_HOME}/bin:${PATH}"
RUN apt-get update && \
  apt-get install -y --no-install-recommends \
  sudo \
  curl \
  vim \
  unzip \
  openjdk-17-jdk \
  build-essential \
  software-properties-common \
  ssh && \
  apt-get clean && \
  rm -rf /var/lib/apt/lists/*

# Dirs
RUN mkdir -p ${SPARK_HOME} ${SPARK_HOME}/spark-events ${NOTEBOOKS_DIR} ${IPYTHON_STARTUP_DIR}

# Java: Spark, Iceberg & Postgres (Iceberg catalog)
WORKDIR ${SPARK_HOME}
RUN curl https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz -o spark-${SPARK_VERSION}-bin-hadoop3.tgz \
  && tar xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory ${SPARK_HOME} --strip-components 1 \
  && rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz
RUN curl https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-${SPARK_MAJOR_VERSION}_2.12/${ICEBERG_VERSION}/iceberg-spark-runtime-${SPARK_MAJOR_VERSION}_2.12-${ICEBERG_VERSION}.jar \
  -Lo ${SPARK_HOME}/jars/iceberg-spark-runtime-${SPARK_MAJOR_VERSION}_2.12-${ICEBERG_VERSION}.jar
RUN curl -s https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/${ICEBERG_VERSION}/iceberg-aws-bundle-${ICEBERG_VERSION}.jar \
  -Lo /opt/spark/jars/iceberg-aws-bundle-${ICEBERG_VERSION}.jar
RUN curl https://repo1.maven.org/maven2/org/postgresql/postgresql/${POSTGRES_CONNECTOR_VERSION}/postgresql-${POSTGRES_CONNECTOR_VERSION}.jar \
  -Lo ${SPARK_HOME}/jars/postgresql-${POSTGRES_CONNECTOR_VERSION}.jar
COPY ./spark-defaults.conf ${SPARK_HOME}/conf/spark-defaults.conf

# Python: Jupyter/Spark
RUN --mount=type=bind,source=./,target=/build pip install -r /build/requirements.txt
COPY ./notebook ${LOCAL_BIN}/
COPY ./ipython/startup/ ${IPYTHON_STARTUP_DIR}/

# Entrypoint: start notebook
COPY ./entrypoint.sh ${LOCAL_BIN}/

# File modes
RUN chmod u+x ${SPARK_HOME}/sbin/* && \
  chmod u+x ${SPARK_HOME}/bin/* && \
  chmod u+x ${LOCAL_BIN}/*

ENTRYPOINT ["entrypoint.sh"]
CMD ["notebook"]

---
- name: Ensure configuration directory exists
  become: true
  ansible.builtin.file:
    path: "{{ spark_config_path }}"
    state: directory
    mode: "u=rwx,g=rx,o=rx"

- name: Ensure Docker image build files are synchronized
  become: true
  ansible.posix.synchronize:
    use_ssh_args: true
    src: ./
    dest: "{{ spark_config_path }}/"
    archive: false
    recursive: true
    delete: true

- name: Build Spark container image
  become: true
  community.docker.docker_image_build:
    name: spark-iceberg
    path: "{{ spark_config_path }}"
    dockerfile: Dockerfile.spark
    rebuild: always

- name: Run Spark controller
  become: true
  community.docker.docker_container:
    name: spark-controller
    image: spark-iceberg
    entrypoint: /bin/bash
    command: "start-master.sh"
    state: started
    detach: true
    restart: true
    restart_policy: unless-stopped
    recreate: true
    comparisons:
      env: strict
    env:
      SPARK_MASTER_PORT: "{{ spark_controller_port | int }}"
      SPARK_MASTER_OPTS: >
        -Dspark.deploy.defaultCores=8
        -Dspark.ui.reverseProxy=true
        -Dspark.ui.reverseProxyUrl=https://{{ top_level_domain }}:{{ spark_controller_ui_https_port }}
      SPARK_NO_DAEMONIZE: "true"
    # Spark requires all elements of a cluster: controller, executor, driver to be able to communicate.
    # Adding an extra layer of container networking complicates this greatly so we attach directly to the host network.
    # Port forwarding at the VM level is still required.
    network_mode: host
    mounts:
      - source: "{{ spark_config_path }}/log4j2.properties"
        target: /opt/spark/conf/log4j2.properties
        type: bind

- name: Run Spark worker
  become: true
  community.docker.docker_container:
    name: spark-worker
    image: spark-iceberg
    entrypoint: "/bin/bash"
    command: "start-worker.sh spark://{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address'] }}:{{ spark_controller_port }}"
    state: started
    detach: true
    restart: true
    restart_policy: unless-stopped
    recreate: true
    comparisons:
      env: strict
    env:
      SPARK_NO_DAEMONIZE: "true"
      SPARK_WORKER_PORT: "{{  spark_worker_ports[0] | int }}"
      SPARK_WORKER_OPTS: "-Dspark.ui.reverseProxy=true -Dspark.ui.reverseProxyUrl=https://{{ top_level_domain }}:{{ spark_controller_ui_https_port }}"
    # See comment in controller configuration for the reasons for this choice.
    network_mode: host
    mounts:
      - source: "{{ spark_config_path }}/log4j2.properties"
        target: /opt/spark/conf/log4j2.properties
        type: bind

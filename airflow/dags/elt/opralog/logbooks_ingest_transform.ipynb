{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f8833-c459-44cf-8a5b-0d848c1b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Mapping, Optional, Sequence\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Source\n",
    "INCOMING_ROOT = \"s3a://landing-isis/opralog/incoming\"\n",
    "OPRALOGDB_TABLES: Mapping[str, Optional[Mapping[str, str]]] = dict(\n",
    "    LOGBOOKS=dict(unique_keys=(\"LOGBOOK_ID\",), partition_by=None),\n",
    "    LOGBOOK_ENTRIES=dict(\n",
    "        unique_keys=(\"LOGBOOK_ID\", \"ENTRY_ID\"), partition_by=None\n",
    "    ),\n",
    "    ENTRIES=dict(unique_keys=[\"ENTRY_ID\"], partition_by=None),\n",
    "    MORE_ENTRY_COLUMNS=dict(\n",
    "        unique_keys=[\"ENTRY_ID\", \"COLUMN_NO\", \"ENTRY_TYPE_ID\"],\n",
    "        partition_by=None,\n",
    "    ),\n",
    "    ADDITIONAL_COLUMNS=dict(\n",
    "        unique_keys=[\"COLUMN_NO\", \"ENTRY_TYPE_ID\"], partition_by=None\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Destination\n",
    "TARGET_CATALOG = \"isis\"\n",
    "TARGET_DB = \"cleaned\"\n",
    "OPRALOG_LOGBOOK_MERGED = \"opralog_logbook_entry\" \n",
    "\n",
    "# \n",
    "load_dotenv()\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"spark://data-accelerator.isis.cclrc.ac.uk:7077\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"S3_ACCESS_KEY\"])\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"S3_ACCESS_SECRET\"])\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47334a95-b757-4edb-91b9-e6935e828dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_DB}\")\n",
    "spark.sql(f\"USE {TARGET_CATALOG}.{TARGET_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef65075-c661-4bd5-ab60-1a0b7aedc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load in temporary tables\n",
    "loadtype = \"full\"\n",
    "ingest_date = \"2024/11/21\"\n",
    "for tablename in OPRALOGDB_TABLES.keys():\n",
    "    sources = f\"{INCOMING_ROOT}/{tablename}/{loadtype}/{ingest_date}/*.parquet\"\n",
    "    print(f\"Ingesting path '{sources}'\")\n",
    "    df = spark.read.parquet(sources)\n",
    "    df.createOrReplaceTempView(tablename)\n",
    "    print(spark.sql(f\"SELECT COUNT(*) FROM {tablename}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16fab3-06d8-4812-b56e-7a781ef48e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "WITH ordered_entries AS (\n",
    "    SELECT ENTRY_ID FROM ENTRIES ORDER BY ENTRY_ID\n",
    ")\n",
    "SELECT FIRST_VALUE(ENTRY_ID), LAST_VALUE(ENTRY_ID) FROM ordered_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f052f-5f24-40ea-a192-4421bde077ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "snapshot = \"\"\"\n",
    "SELECT\n",
    "  CAST(ENTRIES.ENTRY_ID AS LONG) AS entry_id,\n",
    "  CAST(LOGBOOKS.LOGBOOK_ID AS LONG) AS logbook_id,\n",
    "  CAST(ADDITIONAL_COLUMNS.COLUMN_NO AS LONG) AS extra_column_no,\n",
    "  CAST(ADDITIONAL_COLUMNS.ENTRY_TYPE_ID AS LONG) AS extra_column_id,\n",
    "  CAST(LOGBOOK_NAME AS STRING) AS logbook_name,\n",
    "  ENTRY_TIMESTAMP AS entered,\n",
    "  CAST(ENTRY_DESCRIPTION AS STRING) AS description,\n",
    "  CAST(SHADOW_COMMENT AS STRING) AS comment_text,\n",
    "  CAST(COL_TITLE AS STRING) AS column_title,\n",
    "  CAST(COL_DATA AS STRING) AS string_data,\n",
    "  CAST(NUMBER_VALUE AS DOUBLE) AS number_data \n",
    "FROM ENTRIES\n",
    "JOIN LOGBOOK_ENTRIES ON LOGBOOK_ENTRIES.ENTRY_ID = ENTRIES.ENTRY_ID\n",
    "JOIN LOGBOOKS ON LOGBOOKS.LOGBOOK_ID = LOGBOOK_ENTRIES.LOGBOOK_ID\n",
    "LEFT OUTER JOIN MORE_ENTRY_COLUMNS ON MORE_ENTRY_COLUMNS.ENTRY_ID = ENTRIES.ENTRY_ID\n",
    "LEFT OUTER JOIN ADDITIONAL_COLUMNS ON ADDITIONAL_COLUMNS.COLUMN_NO = MORE_ENTRY_COLUMNS.COLUMN_NO AND ADDITIONAL_COLUMNS.ENTRY_TYPE_ID = MORE_ENTRY_COLUMNS.ENTRY_TYPE_ID\n",
    "WHERE\n",
    "  LOGBOOK_ENTRIES.LOGBOOK_ID = PRINCIPAL_LOGBOOK\n",
    "  AND (COL_DATA IS NOT NULL OR NUMBER_VALUE IS NOT NULL)\n",
    "\"\"\"\n",
    "snapshot_df = spark.sql(snapshot)\n",
    "snapshot_df.createOrReplaceTempView(\"snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f4502-7bba-497b-afbd-f1276c6a050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loading a full snapshot should just replace the whole table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {OPRALOG_LOGBOOK_MERGED}\")\n",
    "\n",
    "table_ensure_exists = f\"\"\"\n",
    "CREATE TABLE {OPRALOG_LOGBOOK_MERGED} (\n",
    "  entry_id LONG,\n",
    "  logbook_id LONG,\n",
    "  extra_column_no LONG,\n",
    "  extra_column_id LONG,\n",
    "  logbook_name STRING,\n",
    "  entered TIMESTAMP,\n",
    "  description STRING,\n",
    "  comment_text STRING,\n",
    "  column_title STRING,\n",
    "  string_data STRING,\n",
    "  number_data DOUBLE\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (logbook_name, month(entered))\n",
    "\"\"\"\n",
    "spark.sql(table_ensure_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59145f6-2c4f-41bc-9edb-4ab7d6231560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "insert_into = f\"INSERT INTO {OPRALOG_LOGBOOK_MERGED} SELECT * FROM snapshot\"\n",
    "spark.sql(insert_into)\n",
    "\n",
    "# We will need to use merge when we have incremental snapshots\n",
    "# merge_snapshot = \"\"\"\n",
    "# MERGE INTO opralog_logbook_entry t\n",
    "# USING snapshot s\n",
    "# ON t.logbook_id = s.logbook_id AND t.entry_id = s.entry_id AND t.extra_column_no = s.extra_column_no AND t.extra_column_id = s.extra_column_id\n",
    "# WHEN MATCHED THEN UPDATE SET *\n",
    "# WHEN NOT MATCHED THEN INSERT *\n",
    "# \"\"\"\n",
    "# df = spark.sql(merge_snapshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcb791-372f-447f-99ed-eb7e4b648ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM {OPRALOG_LOGBOOK_MERGED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb95977-fcf6-4976-8931-8690d5cbaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql WITH ordered_entries AS ( \\\n",
    "    SELECT entry_id FROM {OPRALOG_LOGBOOK_MERGED} ORDER BY entry_id \\\n",
    ") \\\n",
    "SELECT FIRST_VALUE(entry_id), LAST_VALUE(entry_id) FROM ordered_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a1f57-b68f-4fa0-afc6-6480db11f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DESCRIBE EXTENDED {OPRALOG_LOGBOOK_MERGED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff94d1-7fa3-4d26-b22d-b3565d3356c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
